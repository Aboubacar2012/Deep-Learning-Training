{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project: CNN-Handwritten Digit Recognition-Larger Convolutional Neural Network for MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMCFvv6xaig1Lg4DG8cIb4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aboubacar2012/Deep-Learning-Training/blob/main/Project_CNN_Handwritten_Digit_Recognition_Larger_Convolutional_Neural_Network_for_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Larger Convolutional Neural Network for MNIST**"
      ],
      "metadata": {
        "id": "bdj2ew-qB7rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have seen how to create a simple CNN, letâ€™s take a look at a model capable of close\n",
        "to state-of-the-art results. We import the classes and functions then load and prepare the data\n",
        "the same as in the previous CNN example. This time we define a larger CNN architecture with\n",
        "additional convolutional, max pooling layers and fully connected layers. The network topology\n",
        "can be summarized as follows."
      ],
      "metadata": {
        "id": "WerBUekqCKco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Convolutional layer with 30 features maps of size 5x5\n",
        "\n",
        "- Pooling layer taking the max over 2X2 patches\n",
        "\n",
        "- Convolutional layer with 15 features maps of 3x3\n",
        "\n",
        "- Pooling layer taking the max over 2x2 patches\n",
        "\n",
        "- Dropout layer with a probability of 20%\n",
        "\n",
        "- Flatten layer.\n",
        "\n",
        "- Fully connected layer with 128 neurons and rectifier activation \n",
        "\n",
        "- Fully connected layer with 50 neurons and rectifier activation. \n",
        "\n",
        "- Output layer. "
      ],
      "metadata": {
        "id": "VVONRz01CMKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7990U4x6BvSF",
        "outputId": "fce9b298-9f24-4028-e408-867930455c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 3s - loss: 0.7032 - accuracy: 0.7746 - val_loss: 0.1948 - val_accuracy: 0.9431 - 3s/epoch - 9ms/step\n",
            "Epoch 2/10\n",
            "300/300 - 2s - loss: 0.2624 - accuracy: 0.9185 - val_loss: 0.1345 - val_accuracy: 0.9586 - 2s/epoch - 6ms/step\n",
            "Epoch 3/10\n",
            "300/300 - 2s - loss: 0.2062 - accuracy: 0.9359 - val_loss: 0.1064 - val_accuracy: 0.9680 - 2s/epoch - 6ms/step\n",
            "Epoch 4/10\n",
            "300/300 - 2s - loss: 0.1767 - accuracy: 0.9442 - val_loss: 0.0934 - val_accuracy: 0.9701 - 2s/epoch - 6ms/step\n",
            "Epoch 5/10\n",
            "300/300 - 2s - loss: 0.1538 - accuracy: 0.9517 - val_loss: 0.0794 - val_accuracy: 0.9755 - 2s/epoch - 7ms/step\n",
            "Epoch 6/10\n",
            "300/300 - 2s - loss: 0.1390 - accuracy: 0.9567 - val_loss: 0.0782 - val_accuracy: 0.9760 - 2s/epoch - 6ms/step\n",
            "Epoch 7/10\n",
            "300/300 - 2s - loss: 0.1266 - accuracy: 0.9599 - val_loss: 0.0674 - val_accuracy: 0.9782 - 2s/epoch - 6ms/step\n",
            "Epoch 8/10\n",
            "300/300 - 2s - loss: 0.1158 - accuracy: 0.9635 - val_loss: 0.0619 - val_accuracy: 0.9816 - 2s/epoch - 6ms/step\n",
            "Epoch 9/10\n",
            "300/300 - 2s - loss: 0.1111 - accuracy: 0.9649 - val_loss: 0.0610 - val_accuracy: 0.9808 - 2s/epoch - 6ms/step\n",
            "Epoch 10/10\n",
            "300/300 - 2s - loss: 0.1030 - accuracy: 0.9666 - val_loss: 0.0582 - val_accuracy: 0.9802 - 2s/epoch - 6ms/step\n",
            "Large CNN Error: 1.98%\n"
          ]
        }
      ],
      "source": [
        "# Larger CNN for the MNIST Dataset \n",
        "# Import classes and functions\n",
        "import numpy \n",
        "from keras.datasets import mnist \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense \n",
        "from keras.layers import Dropout \n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed=7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load data \n",
        "(X_train, y_train), (X_test, y_test)=mnist.load_data()\n",
        "\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "X_train=X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
        "X_test=X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
        "\n",
        "#Normalize input from 0-255 to 0-1\n",
        "X_train=X_train/255\n",
        "X_test=X_test/255\n",
        "\n",
        "#One hot encode outputs\n",
        "y_train=np_utils.to_categorical(y_train)\n",
        "y_test=np_utils.to_categorical(y_test)\n",
        "num_classes=y_test.shape[1]\n",
        "\n",
        "# define the larger model\n",
        "def larger_model():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Convolution2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu', data_format='channels_first'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Convolution2D(15, 3, 3,  activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  # Compile model\n",
        "  model.compile(loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Build the model\n",
        "model=larger_model()\n",
        "\n",
        "# Fit the model \n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# Final evaluation of the model \n",
        "scores=model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GmhjnH67DqlM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}